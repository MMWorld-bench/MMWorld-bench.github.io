<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos">
  <meta name="keywords" content="World Models, Benchmark">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="./static/images/worldevallog.png" alt="WorldEval Logo" style="vertical-align: middle; height: 24px;">
          <h1 class="title is-1 publication-title">MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sheehan1230.github.io/">Xuehai He</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://weixi-feng.github.io/">Weixi Feng*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://kzzheng.github.io/">Kaizhi Zheng*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yujielu10.github.io/">Yujie Lu*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://wanrong-zhu.com/">Wanrong Zhu*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/jiachenli/">Jiachen Li*</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="http://www.yfan.site/">Yue Fan*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en">Jianfeng Wang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/linjie-li/">Linjie Li</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://zyang-ur.github.io/">Zhengyuan Yang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/site/kevinlin311tw/me">Kevin Lin</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://sites.cs.ucsb.edu/~william/">William Yang Wang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://eric-xw.github.io/">Xin Eric Wang</a><sup>1</sup>,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Santa Cruz,</span>
            <span class="author-block"><sup>2</sup>University of California, Santa Barbara,</span>
            <span class="author-block"><sup>3</sup>Microsoft,</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><span class="equal-contribution">* Equal Contribution</span></span>
          </div>
          <!-- <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><b style="color:#e08ba0; font-weight:normal"> <b>Under Review</b> </b></span>
          </div> -->

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->

              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>dataset</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code, to be released</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="150%" src="./static/image/teaser.png">
      <h2 class="subtitle has-text-centered">
        <p style="font-family:Times New Roman"><b>Figure 1. MMWorld covers seven broad disciplines and 69 subdisciplines, focusing on the evaluation of multi-faceted reasoning beyond perception (e.g., explanation, counterfactual thinking, future prediction, domain expertise). On the right is a video sample from the Health & Medicine discipline. </b></p>
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of "world models"---interpreting and reasoning about complex real-world dynamics. 
To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities.
To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding.
MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) <b>multi-discipline</b>, covering various disciplines that often require domain expertise for comprehensive understanding; (2) <b>multi-faceted reasoning</b>, including explanation, counterfactual thinking, future prediction, etc.
MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. 
Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. 
The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Dataset Characteristics </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="100%" src="./static/image/dataset_compare.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 2. Comparison between MMWorld and previous benchmarks for real-world video understanding
                on a variety of criteria. Multi-faced include Explanation (Explain.), Counterfactual Thinking
                (Counter.), Future Prediction (Future.) and Domain Expertise (Domain.) MMWorld is the first
                multi-discipline and multitask video understanding benchmark that covers wider reasoning questions,
                and also included first-party data annotations. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="/home/xuehai/MMWorld-bench.github.io/static/image/question_type.png"> Question Types Distribution</h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">  
            <img id="model" width="100%" src="./static/image/question_type.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 3. The questions in MMWorld primarily evaluate seven understanding and reasoning abilities
                of models to provide correct answers. </b></p>
            </h3>   
        </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Synthetic Data Generation Pipeline </h2> 
      </div>
    </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">     
            <img id="model" width="100%" src="./static/image/pipeline.png">
            <h3 class="subtitle has-text-centered">
              <p style="font-family:Times New Roman"><b>Figure 4. Schematic diagram of the synthetic data generation pipeline in MMWorld. It starts with generating subdiscipline-specific queries, followed by video retrieval from YouTube-8M and YouTube. 
                Keyframes are extracted for visual-based QA generation, and videos are transcribed using an ASR module for audio-based QA generation.</b></p>
            </h3>   


        </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="5%" src="https://cdn-icons-png.flaticon.com/512/5379/5379860.png"> Study on MLLM Performance at Different Difficulty Levels for Average Humans</h2> 
      </div>
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-half">  
        <img id="model" width="100%" src="./static/image/difficulty_level.png">
      </div>
      <div class="column is-half">  
        <img id="model" width="100%" src="./static/image/difficulty_level_by_discipline.png">
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h3 class="subtitle has-text-centered">
          <p style="font-family:Times New Roman"><b>Figure 5. Model performance at different difficulty levels for average humans. Average human
            difficulty levels are defined by 3 turkers’ performance per question: Easy (3/3 correct answers),
            medium (2/3 correct), hard (1/3 correct), and expert (0/3 correct).</b></p>
        </h3>   
      </div>
    </div>
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a rel="license"
            href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a rel="license"
            href="https://gligen.github.io/">GLIGEN</a>, licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
